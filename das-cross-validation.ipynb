{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1370,"sourceType":"datasetVersion","datasetId":738}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross Validation  \n학습 셋으로 최적의 모델 정확도를 끌어내고 검증 셋으로 과적합을 방지하는 방법을 holdout이라 한다. 그런데 하나의 고정된 검증 셋 데이터로만 모델을 훈련시키면 overfitting이 발생할 가능성은 여전히 존재한다.","metadata":{}},{"cell_type":"markdown","source":"### k-Fold Cross Validation  \nk-fold cross validation은 전체 학습 데이터 중 일부를 검증 셋으로 분리하는 과정을 k번 반복하는 방법이다.  \n![cross-validation](https://user-images.githubusercontent.com/26833433/258589390-8d815058-ece8-48b9-a94e-0e1ab53ea0f6.png)  \n일반적으로 k의 수(fold의 수)는 5~10개를 지정하며, k가 클수록 평가의 편중된 정도(bias)는 낮아지지만, 각 fold의 모델별 결과의 분산(variance)이 높을 수 있으므로 k가 무조건 크다고 좋은 것은 아니다.","metadata":{}},{"cell_type":"markdown","source":"### LOOCV(Leave-one-out Cross-validation)  \n기존 k-fold 방식에서 k를 최대화한 방법이다. 즉 k를 전체 관측치 수로 하여, 검증 셋이 관측치 하나가 되는 것이다. 이 방식은 모델의 편중이 매우 작지만 결과값의 분산이 다소 커 overfitting될 여지가 있다. 또한 데이터의 양이 많은 경우는 적합하지 않다.","metadata":{}},{"cell_type":"markdown","source":"### Stratified K-fold Cross Validation  \n기존 k-fold 방식에 stratified sampling 방식을 접목한 기법으로, 데이터의 특정 클래스가 한 곳에 몰리는 상황을 방지할 수 있다. Stratified K-fold 교차검증은 분할셋 안의 클래스 비율이 학습셋 전체의 클래스 비율과 같도록 분리해준다.","metadata":{}},{"cell_type":"markdown","source":"### Nested Cross Validation\n기존 k-fold 방식은 학습셋과 테스트셋 분리를 한번만 하기 때문에, 모델의 성능이 테스트 셋에 크게 의존한다는 문제가 있다. 이러한 문제를 해결하기 위해 학습셋과 테스트셋 검증에도 k-fold 방식을 적용한 것이 Nested CV이다.  \n![NestedCV](https://miro.medium.com/v2/resize:fit:1400/1*5vky1z29e1iO6iOvCTBJxg.png)","metadata":{}},{"cell_type":"markdown","source":"### Grid Search Cross Validation\nGrid Search Cross Validation은 모델의 hyper parameter 값을 리스트 형태로 미리 입력해 놓은 다음 각 조건의 모델 성능을 측정하고 평가하여 효율적으로 최적의 파라미터 값을 찾아내는 기법이다.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, LeaveOneOut, StratifiedKFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-02-12T12:24:57.004112Z","iopub.execute_input":"2024-02-12T12:24:57.004455Z","iopub.status.idle":"2024-02-12T12:24:58.098965Z","shell.execute_reply.started":"2024-02-12T12:24:57.004429Z","shell.execute_reply":"2024-02-12T12:24:58.098042Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/datasets/uciml/glass\ndf = pd.read_csv(\"../input/glass/glass.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-12T12:25:01.084890Z","iopub.execute_input":"2024-02-12T12:25:01.085670Z","iopub.status.idle":"2024-02-12T12:25:01.124077Z","shell.execute_reply.started":"2024-02-12T12:25:01.085638Z","shell.execute_reply":"2024-02-12T12:25:01.123411Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-12T12:25:02.324231Z","iopub.execute_input":"2024-02-12T12:25:02.324833Z","iopub.status.idle":"2024-02-12T12:25:02.345337Z","shell.execute_reply.started":"2024-02-12T12:25:02.324805Z","shell.execute_reply":"2024-02-12T12:25:02.344576Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RI</th>\n      <th>Na</th>\n      <th>Mg</th>\n      <th>Al</th>\n      <th>Si</th>\n      <th>K</th>\n      <th>Ca</th>\n      <th>Ba</th>\n      <th>Fe</th>\n      <th>Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.52101</td>\n      <td>13.64</td>\n      <td>4.49</td>\n      <td>1.10</td>\n      <td>71.78</td>\n      <td>0.06</td>\n      <td>8.75</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.51761</td>\n      <td>13.89</td>\n      <td>3.60</td>\n      <td>1.36</td>\n      <td>72.73</td>\n      <td>0.48</td>\n      <td>7.83</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.51618</td>\n      <td>13.53</td>\n      <td>3.55</td>\n      <td>1.54</td>\n      <td>72.99</td>\n      <td>0.39</td>\n      <td>7.78</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.51766</td>\n      <td>13.21</td>\n      <td>3.69</td>\n      <td>1.29</td>\n      <td>72.61</td>\n      <td>0.57</td>\n      <td>8.22</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.51742</td>\n      <td>13.27</td>\n      <td>3.62</td>\n      <td>1.24</td>\n      <td>73.08</td>\n      <td>0.55</td>\n      <td>8.07</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# k-fold 방법 1\n\n# 종속변수 문자형 변환\ndf[\"Type_str\"]=df[\"Type\"].apply(str)\n\n# 독립변수, 종속변수 분리\ndf_x1 = df[['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe']]\ndf_y1 = df[['Type_str']]\n\n# 기본 모델 설정\nrnf_model = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=0)\n\n# 학습셋과 테스트셋 분리하여 생성(6:4)\nx_train, x_test, y_train, y_test = train_test_split(\n    df_x1,df_y1,test_size=0.4,random_state=10)\n\n# cross_val_score 함수로 k-fold 성능 측정\nkfold_scores_1 = cross_val_score(rnf_model, x_train, y_train, cv = 7)\n\nprint('k-fold 교차 검증 SCORE : ', kfold_scores_1)\nprint(\"k-fold 교차 검증 SCORE 평균 : {:.2f}\".format(kfold_scores_1.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-02-12T12:25:06.889853Z","iopub.execute_input":"2024-02-12T12:25:06.890176Z","iopub.status.idle":"2024-02-12T12:25:07.880532Z","shell.execute_reply.started":"2024-02-12T12:25:06.890153Z","shell.execute_reply":"2024-02-12T12:25:07.879692Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=7.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n","output_type":"stream"},{"name":"stdout","text":"k-fold 교차 검증 SCORE :  [0.89473684 0.68421053 0.61111111 0.55555556 0.94444444 0.72222222\n 0.88888889]\nk-fold 교차 검증 SCORE 평균 : 0.76\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n","output_type":"stream"}]},{"cell_type":"code","source":"# k-fold 방법 2\n\n# 종속변수 문자형 변환 및 분리\ndf[\"Type_str\"]=df[\"Type\"].apply(str)\ndf_x2 = np.array(df[['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe']])\ndf_y2 = df['Type_str']\n\n# k-fold 설정\nkfold = KFold(n_splits=7, shuffle = True, random_state = 37)\nkfold_scores_2_mean = []\nfold_idx = 0\n\n# k-fold 수행\nfor train_idx, test_idx in kfold.split(df_x2):\n    \n    train_x, train_y = df_x2[train_idx], df_y2[train_idx]\n    test_x, test_y = df_x2[test_idx], df_y2[test_idx]\n    \n    # 기본 모델 설정\n    rnf_model = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=0)    \n    rnf_model.fit(train_x, train_y)\n    pred_y = rnf_model.predict(test_x)\n    kfold_scores_2 = accuracy_score(test_y, pred_y)    \n    fold_idx += 1    \n    kfold_scores_2_mean.append(kfold_scores_2)\n\nprint(f\"k-fold 교차 검증 SCORE 평균 : {np.mean(kfold_scores_2_mean)}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-12T12:25:12.940071Z","iopub.execute_input":"2024-02-12T12:25:12.940914Z","iopub.status.idle":"2024-02-12T12:25:13.956309Z","shell.execute_reply.started":"2024-02-12T12:25:12.940884Z","shell.execute_reply":"2024-02-12T12:25:13.955583Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"k-fold 교차 검증 SCORE 평균 : 0.7526881720430108\n","output_type":"stream"}]},{"cell_type":"code","source":"# LOOCV (Leave-one-out Cross-validation)\n\n# 기본 모델 설정\nrnf_model = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=0)\nrnf_model.fit(train_x, train_y)\npred_y = rnf_model.predict(test_x)\n\nloocv = LeaveOneOut()\nloocv_scores = cross_val_score(rnf_model,test_x,test_y, cv = loocv)\n\nprint(\"테스트 셋 전체 관측치 수 : \", len(test_y))\nprint(\"LOOCV 검증 분할 횟수 : \", len(loocv_scores))\nprint(\"LOOCV 교차 검증 SCORE 평균 : {:.2f}\".format(loocv_scores.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-02-12T12:25:17.648872Z","iopub.execute_input":"2024-02-12T12:25:17.649541Z","iopub.status.idle":"2024-02-12T12:25:21.459725Z","shell.execute_reply.started":"2024-02-12T12:25:17.649514Z","shell.execute_reply":"2024-02-12T12:25:21.458866Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"테스트 셋 전체 관측치 수 :  30\nLOOCV 검증 분할 횟수 :  30\nLOOCV 교차 검증 SCORE 평균 : 0.70\n","output_type":"stream"}]},{"cell_type":"code","source":"# Stratified K-fold Cross Validation\n\n# Stratified K-fold 교차검증 설정\nstrati = StratifiedKFold(n_splits=3)\n\nn_iter=0\nprint(\"전체 데이터셋 범주 별 관측치 수 :\\n\",df_y2.value_counts())\nfor train_index, test_index in strati.split(df_x2,df_y2):\n    n_iter +=1\n    strati_train_y = df_y2.iloc[train_index]\n    strati_test_y = df_y2.iloc[test_index]\n    print('분할 {0}'.format(n_iter))\n    print('학습 셋 범주 별 관측치 수:\\n', strati_train_y.value_counts())\n    print('검증 셋 범주 별 관측치 수:\\n', strati_test_y.value_counts())\n\nstrati_scores = cross_val_score(rnf_model,test_x,test_y, cv = strati)\nprint(\"Stratified K-fold 교차 검증 SCORE 평균 : {:.2f}\".format(strati_scores.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-02-12T12:25:25.180559Z","iopub.execute_input":"2024-02-12T12:25:25.181143Z","iopub.status.idle":"2024-02-12T12:25:25.565580Z","shell.execute_reply.started":"2024-02-12T12:25:25.181108Z","shell.execute_reply":"2024-02-12T12:25:25.564812Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"전체 데이터셋 범주 별 관측치 수 :\n Type_str\n2    76\n1    70\n7    29\n3    17\n5    13\n6     9\nName: count, dtype: int64\n분할 1\n학습 셋 범주 별 관측치 수:\n Type_str\n2    51\n1    46\n7    19\n3    11\n5     9\n6     6\nName: count, dtype: int64\n검증 셋 범주 별 관측치 수:\n Type_str\n2    25\n1    24\n7    10\n3     6\n5     4\n6     3\nName: count, dtype: int64\n분할 2\n학습 셋 범주 별 관측치 수:\n Type_str\n2    50\n1    47\n7    20\n3    12\n5     8\n6     6\nName: count, dtype: int64\n검증 셋 범주 별 관측치 수:\n Type_str\n2    26\n1    23\n7     9\n3     5\n5     5\n6     3\nName: count, dtype: int64\n분할 3\n학습 셋 범주 별 관측치 수:\n Type_str\n2    51\n1    47\n7    19\n3    11\n5     9\n6     6\nName: count, dtype: int64\n검증 셋 범주 별 관측치 수:\n Type_str\n2    25\n1    23\n7    10\n3     6\n5     4\n6     3\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Stratified K-fold 교차 검증 SCORE 평균 : 0.67\n","output_type":"stream"}]},{"cell_type":"code","source":"# Nested Cross Validation 및 Grid Search Cross Validation\n\n# 학습셋과 테스트셋 분리하여 생성(6:4)\nx_train, x_test, y_train, y_test = train_test_split(\n    df_x2,df_y2,test_size=0.4,random_state=10)\n\n# 모델 시도 횟수 설정\nNUM_TRIALS = 20\n\n# 그리드 하이퍼 파라미터 설정\nhp_para = {'max_depth':[2,3,4], 'min_samples_split':[2,3]}\n\n# 스코어를 array로 저장\nnested_scores = np.zeros(NUM_TRIALS)\n\n# 교차 검증 반복 수행\nfor i in range(NUM_TRIALS):\n\n    inner_loop = KFold(n_splits=3, shuffle=True, random_state=i)\n    outer_loop = KFold(n_splits=3, shuffle=True, random_state=i)\n\n    # Nested Cross Validation 하이퍼 파라미터 최적화\n    gs_cv = GridSearchCV(rnf_model, param_grid=hp_para, cv=inner_loop)\n    nested_score = cross_val_score(gs_cv, X=x_train, y=y_train, cv=outer_loop)\n    nested_scores[i] = nested_score.mean()\n\n# 테스트셋에 모델 적용\ngs_cv.fit(x_test, y_test)\n\nprint(\"각 TRIAL 별 SCORE 평균 : \\n\", nested_scores)\nprint(\"전체 TRIAL SCORE 평균 : {:.2f}\".format(nested_scores.mean()))\nprint(\"최대 TRIAL SCORE : {:.2f}\".format(nested_scores.max()))\nprint(\"최적 하이퍼 파라미터 : \\n\", gs_cv.best_params_)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T12:25:34.353109Z","iopub.execute_input":"2024-02-12T12:25:34.353458Z","iopub.status.idle":"2024-02-12T12:28:00.130895Z","shell.execute_reply.started":"2024-02-12T12:25:34.353433Z","shell.execute_reply":"2024-02-12T12:28:00.130030Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"각 TRIAL 별 SCORE 평균 : \n [0.71059432 0.71077889 0.67884828 0.72646733 0.71834625 0.69545958\n 0.72683647 0.68752307 0.6720192  0.68014027 0.72757475 0.7113326\n 0.68014027 0.7113326  0.66426726 0.68807678 0.73366556 0.73421927\n 0.67109635 0.70302695]\n전체 TRIAL SCORE 평균 : 0.70\n최대 TRIAL SCORE : 0.73\n최적 하이퍼 파라미터 : \n {'max_depth': 4, 'min_samples_split': 2}\n","output_type":"stream"}]}]}